{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pyspark_project_MDI701_NOUBOUE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFxh7H5aNrrW"
      },
      "source": [
        "# Music Recommender System using ALS Algorithm with Apache Spark and Python\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khgJ0rTxhZys"
      },
      "source": [
        "## /!\\ Disclaimer : Installation /!\\ \r\n",
        "This notebook is run on google Colab environment.\r\n",
        "\r\n",
        "To run this notebook, you need to have spark installed on your PC. If not, To run it on Google colab, you have first to install Java and Spark by executing the following cell.\r\n",
        "\r\n",
        "If you already have spark correctly installed, you can skip it, and go directlty to the next section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1jHCK3Jhltn"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "!wget -q https://apache.mediamirrors.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\r\n",
        "!tar xf spark-3.0.1-bin-hadoop3.2.tgz\r\n",
        "!pip3 install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d46HNxC1P6Kh"
      },
      "source": [
        "import os\r\n",
        "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n4OdTFYNxOP"
      },
      "source": [
        "## Description: \r\n",
        "\r\n",
        "The goal of this project is to create a recommender system that will recommend new musical artists to a user based on their listening history.\r\n",
        "\r\n",
        "To achieve it, we will use Spark and the collaborative filtering technique. We will also use built-in functions provided with spark API. This technique can be also use to build a film recommander system.\r\n",
        "\r\n",
        "\r\n",
        "### Dataset : \r\n",
        "\r\n",
        "We will be using some publicly available song data from audioscrobbler. However, we modified the original data files so that the code will run in a reasonable time on a single machine. The reduced data files contains only the information relevant to the top 50 most prolific users (highest artist play counts).\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBb6SE-ljwvT"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXjbOXVeyg7E"
      },
      "source": [
        "# Import libraries\r\n",
        "import findspark\r\n",
        "findspark.init()\r\n",
        "\r\n",
        "from pyspark.mllib.recommendation import *\r\n",
        "import random\r\n",
        "from operator import *\r\n",
        "from collections import defaultdict"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uefvprwVj2_b"
      },
      "source": [
        "## Initialize Spark context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOxYGP8uVjKn"
      },
      "source": [
        "# Initialize Spark Context\r\n",
        "from pyspark import SparkContext, SparkConf\r\n",
        "spark = SparkContext.getOrCreate()\r\n",
        "spark.stop()\r\n",
        "spark = SparkContext('local','Recommender')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81Yn-BPwkJGS"
      },
      "source": [
        "Load the three datasets into RDDs and name them artistData, artistAlias, and userArtistData."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GviBV9mrVm5v"
      },
      "source": [
        "artistData = spark.textFile('/content/drive/MyDrive/Colab Datasets/data_spark/artist_data_small.txt').map(lambda s:(int(s.split(\"\\t\")[0]),s.split(\"\\t\")[1]))\r\n",
        "artistAlias = spark.textFile('/content/drive/MyDrive/Colab Datasets/data_spark/artist_alias_small.txt')\r\n",
        "userArtistData = spark.textFile('/content/drive/MyDrive/Colab Datasets/data_spark/user_artist_data_small.txt')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r8FJL3ckXFz"
      },
      "source": [
        "## Data Exploration:\r\n",
        "\r\n",
        "In the next section we will find the users' total play counts. Then find and print the three users with the highest number of total play counts (sum of all counters)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzVolGEhXzSS",
        "outputId": "109e4f62-c255-487e-ebfb-23bd0b93723e"
      },
      "source": [
        "# Split a sequence into seperate entities and store as int\r\n",
        "userArtistData = userArtistData.map(lambda s:(int(s.split(\" \")[0]),int(s.split(\" \")[1]),int(s.split(\" \")[2])))\r\n",
        "\r\n",
        "# Create a dictionary of the 'artistAlias' dataset\r\n",
        "artistAliasDictionary = {}\r\n",
        "dataValue = artistAlias.map(lambda s:(int(s.split(\"\\t\")[0]),int(s.split(\"\\t\")[1])))\r\n",
        "for temp in dataValue.collect():\r\n",
        "    artistAliasDictionary[temp[0]] = temp[1]\r\n",
        "\r\n",
        "# If artistid exists, replace with artistsid from artistAlias, else retain original\r\n",
        "userArtistData = userArtistData.map(lambda x: (x[0], artistAliasDictionary[x[1]] if x[1] in artistAliasDictionary else x[1], x[2]))\r\n",
        "\r\n",
        "userSum = userArtistData.map(lambda x:(x[0],x[2]))\r\n",
        "playCount1 = userSum.map(lambda x: (x[0],x[1])).reduceByKey(lambda a,b : a+b)\r\n",
        "playCount2 = userSum.map(lambda x: (x[0],1)).reduceByKey(lambda a,b:a+b)\r\n",
        "playSumAndCount = playCount1.leftOuterJoin(playCount2)\r\n",
        "\r\n",
        "\r\n",
        "# Count instances by key and store in broadcast variable\r\n",
        "playSumAndCount = playSumAndCount.map(lambda x: (x[0],x[1][0],int(x[1][0]/x[1][1])))\r\n",
        "\r\n",
        "# Compute and display users with the highest playcount along with their mean playcount across artists\r\n",
        "TopThree = playSumAndCount.top(3,key=lambda x: x[1])\r\n",
        "for i in TopThree:\r\n",
        "    print('User '+str(i[0])+' has a total play count of '+str(i[1])+' and a mean play count of '+str(i[2])+'.')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User 1059637 has a total play count of 674412 and a mean play count of 1878.\n",
            "User 2064012 has a total play count of 548427 and a mean play count of 9455.\n",
            "User 2069337 has a total play count of 393515 and a mean play count of 1519.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ara-lHulms9"
      },
      "source": [
        "## Splitting Data for Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6HdMrMcX4cg",
        "outputId": "484076ba-8478-4bfc-acb5-c2151892d982"
      },
      "source": [
        "# Split the 'userArtistData' dataset into training, validation and test datasets. Store in cache for frequent access\r\n",
        "trainData, validationData, testData = userArtistData.randomSplit((0.4,0.4,0.2),seed=16)\r\n",
        "trainData.cache()\r\n",
        "validationData.cache()\r\n",
        "testData.cache()\r\n",
        "\r\n",
        "# Display the first 3 records of each dataset followed by the total count of records for each datasets\r\n",
        "print(trainData.take(3))\r\n",
        "print(validationData.take(3))\r\n",
        "print(testData.take(3))\r\n",
        "print(trainData.count())\r\n",
        "print(validationData.count())\r\n",
        "print(testData.count())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1059637, 1000010, 238), (1059637, 1000049, 1), (1059637, 1000056, 1)]\n",
            "[(1059637, 1000094, 1), (1059637, 1000113, 5), (1059637, 1000130, 19129)]\n",
            "[(1059637, 1000062, 11), (1059637, 1000112, 423), (1059637, 1000427, 20)]\n",
            "19764\n",
            "19854\n",
            "9863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12-VpAwCl2y-"
      },
      "source": [
        "## The Recommender Model\r\n",
        "For this project, we will train the model with implicit feedback. You can read more information about this from the <strong>collaborative filtering</strong> page: http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html. \r\n",
        "\r\n",
        "To find the best model's parameters combination, we will do a small parameter sweep and choose the model that performs the best on the validation set.\r\n",
        "\r\n",
        "Therefore, we must first devise a way to evaluate models. Once we have a method for evaluation, we can run a parameter sweep, evaluate each combination of parameters on the validation data, and choose the optimal set of parameters. The parameters then can be used to make predictions on the test data.\r\n",
        "\r\n",
        "### Model Evaluation\r\n",
        "To evaluate a model, we will compare the fraction of overlap between the top X predictions of the model and the X artists that the user actually listened to. This process can be repeated for all users and an average value returned.\r\n",
        "\r\n",
        "Our function will be named function `modelEval` and will take a model (the output of ALS.trainImplicit) and a dataset as input. The model will be evaluated on the test data (testData)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g55iMf3X-24"
      },
      "source": [
        "def modelEval(model, dataset):\r\n",
        "    \r\n",
        "    # All artists in the 'userArtistData' dataset\r\n",
        "    AllArtists = spark.parallelize(set(userArtistData.map(lambda x:x[1]).collect()))\r\n",
        "    \r\n",
        "    \r\n",
        "    # Set of all users in the current (Validation/Testing) dataset\r\n",
        "    AllUsers = spark.parallelize(set(dataset.map(lambda x:x[0]).collect()))\r\n",
        "    \r\n",
        "    \r\n",
        "    # Create a dictionary of (key, values) for current (Validation/Testing) dataset\r\n",
        "    ValidationAndTestingDictionary ={}\r\n",
        "    for temp in AllUsers.collect():\r\n",
        "        tempFilter = dataset.filter(lambda x:x[0] == temp).collect()\r\n",
        "        for item in tempFilter:\r\n",
        "            if temp in ValidationAndTestingDictionary:\r\n",
        "                ValidationAndTestingDictionary[temp].append(item[1])\r\n",
        "            else:\r\n",
        "                ValidationAndTestingDictionary[temp] = [item[1]]\r\n",
        "                    \r\n",
        "    \r\n",
        "    # Create a dictionary of (key, values) for training dataset\r\n",
        "    TrainingDictionary = {}\r\n",
        "    for temp in AllUsers.collect():\r\n",
        "        tempFilter = trainData.filter(lambda x:x[0] == temp).collect()\r\n",
        "        for item in tempFilter:\r\n",
        "            if temp in TrainingDictionary:\r\n",
        "                TrainingDictionary[temp].append(item[1])\r\n",
        "            else:\r\n",
        "                TrainingDictionary[temp] = [item[1]]\r\n",
        "        \r\n",
        "    \r\n",
        "    # Calculate the prediction score for each user\r\n",
        "    PredictionScore = 0.00\r\n",
        "    for temp in AllUsers.collect():\r\n",
        "        ArtistPrediction =  AllArtists.map(lambda x:(temp,x))\r\n",
        "        ModelPrediction = model.predictAll(ArtistPrediction)\r\n",
        "        tempFilter = ModelPrediction.filter(lambda x :not x[1] in TrainingDictionary[x[0]])\r\n",
        "        topPredictions = tempFilter.top(len(ValidationAndTestingDictionary[temp]),key=lambda x:x[2])\r\n",
        "        l=[]\r\n",
        "        for i in topPredictions:\r\n",
        "            l.append(i[1])\r\n",
        "        PredictionScore+=len(set(l).intersection(ValidationAndTestingDictionary[temp]))/len(ValidationAndTestingDictionary[temp])    \r\n",
        "\r\n",
        "        \r\n",
        "    # Print average score of the model for all users for the specified rank\r\n",
        "    print(\"The model score for rank \"+str(model.rank)+\" is ~\"+str(PredictionScore/len(ValidationAndTestingDictionary)))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4btyxIkjonpS"
      },
      "source": [
        "### Model Construction\r\n",
        "\r\n",
        "Now we will just try a few different values for the rank parameter. We coud do it better latter. We will loop through the values [2, 8, 10, 15, 20] and figure out which one produces the highest scored based on your model evaluation function.\r\n",
        "\r\n",
        "Note: this procedure may take several minutes to run.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAL7liqsYEaA",
        "outputId": "7e9df3e5-e519-4076-d7dc-b4efabde206b"
      },
      "source": [
        "rankList = [2, 8, 10, 11, 15]\r\n",
        "for rank in rankList:\r\n",
        "  model = ALS.trainImplicit(trainData, rank , seed=345)\r\n",
        "  modelEval(model,validationData)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model score for rank 2 is ~0.08271570722731275\n",
            "The model score for rank 8 is ~0.08649870403564805\n",
            "The model score for rank 10 is ~0.09606702451185019\n",
            "The model score for rank 11 is ~0.09248519196233718\n",
            "The model score for rank 15 is ~0.09133240312585972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sABx7TQ8acxN",
        "outputId": "624146bb-58e9-42b9-9f9c-5865697be93a"
      },
      "source": [
        "bestModel = ALS.trainImplicit(trainData, rank=10, seed=345)\r\n",
        "modelEval(bestModel, testData)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model score for rank 10 is ~0.058094084259049265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ehWd4L3pc0Y"
      },
      "source": [
        "## Trying Some Artist Recommendations\r\n",
        "Using the best model above, predict the top 5 artists for user 1059637 using the <strong>recommendProducts</strong> function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srueul4mYRQX",
        "outputId": "7e4cae56-2868-4910-cfbe-109dd480b4f6"
      },
      "source": [
        "# Top 5 artists for a particular user and list their names\r\n",
        "TopFive = bestModel.recommendProducts(1059637,5)\r\n",
        "for item in range(0,5):\r\n",
        "    print(\"Artist \"+str(item)+\": \"+artistData.filter(lambda x:x[0] == TopFive[item][1]).collect()[0][1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Artist 0: My Chemical Romance\n",
            "Artist 1: Evanescence\n",
            "Artist 2: Brand New\n",
            "Artist 3: Death Cab for Cutie\n",
            "Artist 4: Avril Lavigne\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCvhXNoytpq1"
      },
      "source": [
        "Now we will predict the top 5 artists for user **2069337** ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH41ec4ouRLL",
        "outputId": "027860ec-044c-4044-d7b7-c1b9f162afe9"
      },
      "source": [
        "# Top 5 artists for a particular user and list their names\r\n",
        "TopFive = bestModel.recommendProducts(2069337,5)\r\n",
        "for item in range(0,5):\r\n",
        "    print(\"Artist \"+str(item)+\": \"+artistData.filter(lambda x:x[0] == TopFive[item][1]).collect()[0][1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Artist 0: Give up the Ghost\n",
            "Artist 1: Whatever It Takes\n",
            "Artist 2: Keepsake\n",
            "Artist 3: Kid Dynamite\n",
            "Artist 4: Counter Action\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}